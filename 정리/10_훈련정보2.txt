#===(1)====================================================================================================
batch = 64
lr = 0.001

output_dir = r'C:\code\python\autohunting\dataset_TFRecord'
train_tfrecord_filename = '1day50seq23feature_TRAIN.tfrecord'   -> standard
val_tfrecord_filename = '1day50seq23feature_VAL.tfrecord'
sequence_length = 50  # 시퀀스 길이
feature_dim = 38  # 한 샘플의 특성 수 (레이블 제외)

def transformer_lstm_model(input_shape):
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = PositionEncoding()(inputs)
    attn_output1 = tf.keras.layers.MultiHeadAttention(key_dim=128, num_heads=16)(x, x)
    attn_output2 = tf.keras.layers.LayerNormalization(epsilon=1e-8)(attn_output1 + x)  # 잔차 연결
    x_ff1 = tf.keras.layers.Dense(512, activation='tanh')(attn_output2) 
    x_ff2 = tf.keras.layers.Dense(38, activation='tanh')(x_ff1) 
    # x_ff3 = tf.keras.layers.Dense(23, activation='tanh')(x_ff2)
    dense_out = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x_ff2 + attn_output1 + x)  # 잔차 연결
    lstm_out = tf.keras.layers.LSTM(128)(dense_out)
    # lstm_out = tf.keras.layers.GlobalMaxPooling1D()(dense_out)  # 시퀀스 차원을 축소
    output = tf.keras.layers.Dense(1, activation='sigmoid')(lstm_out)

    return tf.keras.models.Model(inputs, output)




    Epoch 81/100
23773/23773 [==============================] - 520s 22ms/step - loss: 0.4528 - accuracy: 0.7796 - val_loss: 0.6128 - val_accuracy: 0.7177
Epoch 82/100
23773/23773 [==============================] - 527s 22ms/step - loss: 0.4524 - accuracy: 0.7798 - val_loss: 0.6272 - val_accuracy: 0.7126
Epoch 83/100
23773/23773 [==============================] - 526s 22ms/step - loss: 0.4530 - accuracy: 0.7793 - val_loss: 0.6266 - val_accuracy: 0.7097
Epoch 84/100
23773/23773 [==============================] - 526s 22ms/step - loss: 0.4516 - accuracy: 0.7801 - val_loss: 0.6132 - val_accuracy: 0.7088
Epoch 85/100
23773/23773 [==============================] - 532s 22ms/step - loss: 0.4522 - accuracy: 0.7799 - val_loss: 0.6178 - val_accuracy: 0.7053
Epoch 86/100
23773/23773 [==============================] - 528s 22ms/step - loss: 0.4524 - accuracy: 0.7795 - val_loss: 0.6213 - val_accuracy: 0.7062
Epoch 87/100
23773/23773 [==============================] - 557s 23ms/step - loss: 0.4520 - accuracy: 0.7800 - val_loss: 0.6174 - val_accuracy: 0.7125
Epoch 88/100