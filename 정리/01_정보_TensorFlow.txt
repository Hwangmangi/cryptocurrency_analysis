●1. layer설명
●2. tf.data.Dataset 사용
●3. TFRecord


포지셔널 인코딩
포지셔널 인코딩 값은 입력 데이터 크기의 1%에서 10% 사이로 설정하는 것이 적당합니다.

x = NormalizeByFirstRow()(inputs)
(inputs) 전달: NormalizeByFirstRow() 인스턴스를 호출하는 시점에서, inputs 텐서가 call 메소드의 매개변수로 전달됩니다.
 즉, inputs는 (batch_size, 10, 5) 크기의 텐서로, NormalizeByFirstRow 레이어에 들어가게 됩니다.
====================================================================================================
●1. layer설명
tf.keras.layers.LSTM(
    units,  # 출력 공간의 차원 (즉, LSTM 유닛의 수)
    activation='tanh',  # 활성화 함수 (기본값: 'tanh')
    recurrent_activation='sigmoid',  # 순환 단계의 활성화 함수 (기본값: 'sigmoid')
    use_bias=True,  # 바이어스 사용 여부 (기본값: True)
    kernel_initializer='glorot_uniform',  # 커널(가중치) 초기화 방법 (기본값: 'glorot_uniform')
    recurrent_initializer='orthogonal',  # 순환 커널 초기화 방법 (기본값: 'orthogonal')
    bias_initializer='zeros',  # 바이어스 초기화 방법 (기본값: 'zeros')
    unit_forget_bias=True,  # 잊음 바이어스 사용 여부 (기본값: True)
    kernel_regularizer=None,  # 커널 정규화 방법 (기본값: None)
    recurrent_regularizer=None,  # 순환 커널 정규화 방법 (기본값: None)
    bias_regularizer=None,  # 바이어스 정규화 방법 (기본값: None)
    activity_regularizer=None,  # 출력 정규화 방법 (기본값: None)
    kernel_constraint=None,  # 커널 제약 조건 (기본값: None)
    recurrent_constraint=None,  # 순환 커널 제약 조건 (기본값: None)
    bias_constraint=None,  # 바이어스 제약 조건 (기본값: None)
    dropout=0.0,  # 입력에 대한 드롭아웃 비율 (기본값: 0.0)
    recurrent_dropout=0.0,  # 순환 상태에 대한 드롭아웃 비율 (기본값: 0.0)
    return_sequences=False,  # 시퀀스 출력 여부 (기본값: False)
    return_state=False,  # 마지막 상태 출력 여부 (기본값: False)
    go_backwards=False,  # 시퀀스를 역방향으로 처리할지 여부 (기본값: False)
    stateful=False,  # 상태 유지 여부 (기본값: False)
    time_major=False,  # 입력의 시간 차원이 첫 번째 차원인지 여부 (기본값: False)
    unroll=False  # RNN을 펼칠지 여부 (기본값: False)
)
1. tf.keras.layers.MultiHeadAttention
tf.keras.layers.MultiHeadAttention(
    num_heads,  # 주의 메커니즘의 헤드 수
    key_dim,  # 각 헤드의 차원
    value_dim=None,  # 값 벡터의 차원 (기본값: None)
    dropout=0.0,  # 드롭아웃 비율 (기본값: 0.0)
    use_bias=True,  # 바이어스 사용 여부 (기본값: True)
    output_shape=None,  # 출력 형태 (기본값: None)
    attention_axes=None,  # 주의 메커니즘이 적용될 축 (기본값: None)
    kernel_initializer='glorot_uniform',  # 커널 초기화 방법 (기본값: 'glorot_uniform')
    bias_initializer='zeros',  # 바이어스 초기화 방법 (기본값: 'zeros')
    kernel_regularizer=None,  # 커널 정규화 방법 (기본값: None)
    bias_regularizer=None,  # 바이어스 정규화 방법 (기본값: None)
    activity_regularizer=None,  # 출력 정규화 방법 (기본값: None)
    kernel_constraint=None,  # 커널 제약 조건 (기본값: None)
    bias_constraint=None  # 바이어스 제약 조건 (기본값: None)
)
설명: 여러 개의 주의 메커니즘 헤드를 사용하여 입력의 다양한 부분에 주의를 기울이는 층입니다. 이는 모델이 입력의 다양한 부분에서 정보를 추출할 수 있게 합니다.

2. tf.keras.layers.Dropout
tf.keras.layers.Dropout(
    rate,  # 드롭아웃 비율
    noise_shape=None,  # 드롭아웃을 적용할 입력의 형태 (기본값: None)
    seed=None  # 랜덤 시드 (기본값: None)
)
설명: 과적합을 방지하기 위해 학습 중에 무작위로 일부 뉴런을 비활성화하는 층입니다.

3. tf.keras.layers.LayerNormalization
tf.keras.layers.LayerNormalization(
    axis=-1,  # 정규화할 축 (기본값: -1)
    epsilon=1e-6,  # 수치 안정성을 위한 작은 상수 (기본값: 1e-6)
    center=True,  # 이동 파라미터를 사용할지 여부 (기본값: True)
    scale=True,  # 스케일 파라미터를 사용할지 여부 (기본값: True)
    beta_initializer='zeros',  # 이동 파라미터 초기화 방법 (기본값: 'zeros')
    gamma_initializer='ones',  # 스케일 파라미터 초기화 방법 (기본값: 'ones')
    beta_regularizer=None,  # 이동 파라미터 정규화 방법 (기본값: None)
    gamma_regularizer=None,  # 스케일 파라미터 정규화 방법 (기본값: None)
    beta_constraint=None,  # 이동 파라미터 제약 조건 (기본값: None)
    gamma_constraint=None  # 스케일 파라미터 제약 조건 (기본값: None)
)
설명: 입력 데이터를 정규화하여 학습을 안정화하고 가속화하는 층입니다.

4. tf.keras.layers.Dense
tf.keras.layers.Dense(
    units,  # 출력 공간의 차원 (즉, 뉴런의 수)
    activation=None,  # 활성화 함수 (기본값: None)
    use_bias=True,  # 바이어스 사용 여부 (기본값: True)
    kernel_initializer='glorot_uniform',  # 커널 초기화 방법 (기본값: 'glorot_uniform')
    bias_initializer='zeros',  # 바이어스 초기화 방법 (기본값: 'zeros')
    kernel_regularizer=None,  # 커널 정규화 방법 (기본값: None)
    bias_regularizer=None,  # 바이어스 정규화 방법 (기본값: None)
    activity_regularizer=None,  # 출력 정규화 방법 (기본값: None)
    kernel_constraint=None,  # 커널 제약 조건 (기본값: None)
    bias_constraint=None  # 바이어스 제약 조건 (기본값: None)
)
설명: 완전 연결 층으로, 입력 데이터에 선형 변환을 적용하고 활성화 함수를 통해 비선형성을 추가합니다.

5. tf.keras.layers.Flatten
tf.keras.layers.Flatten(
    data_format=None  # 데이터 형식 (기본값: None)
)
설명: 입력 데이터를 1차원으로 평탄화하는 층입니다. 주로 Dense 층에 연결하기 전에 사용됩니다.

6. tf.keras.Model
tf.keras.Model(
    inputs,  # 모델의 입력 텐서
    outputs,  # 모델의 출력 텐서
    name=None  # 모델 이름 (기본값: None)
)
설명: 케라스 모델을 정의하는 클래스입니다. 입력과 출력을 지정하여 모델을 구성합니다.

7. tf.keras.Input
tf.keras.Input(
    shape=None,  # 입력 텐서의 형태
    batch_size=None,  # 배치 크기 (기본값: None)
    name=None,  # 입력 이름 (기본값: None)
    dtype=None,  # 데이터 타입 (기본값: None)
    sparse=False,  # 희소 행렬 여부 (기본값: False)
    tensor=None,  # 기존 텐서 (기본값: None)
    ragged=False  # 불규칙 텐서 여부 (기본값: False)
)
설명: 모델의 입력 텐서를 정의하는 함수입니다.

8. tf.keras.layers.Bidirectional
tf.keras.layers.Bidirectional(
    layer,  # 양방향으로 감쌀 레이어 (예: LSTM)
    merge_mode='concat',  # 병합 모드 (기본값: 'concat')
    backward_layer=None  # 역방향 레이어 (기본값: None)
)
설명: 순환 신경망(RNN) 층을 양방향으로 감싸서 순방향과 역방향 모두에서 정보를 학습할 수 있게 합니다.

9. tf.keras.layers.Conv1D
tf.keras.layers.Conv1D(
    filters,  # 출력 공간의 차원 (즉, 필터의 수)
    kernel_size,  # 커널(필터)의 크기
    strides=1,  # 스트라이드 크기 (기본값: 1)
    padding='valid',  # 패딩 방법 (기본값: 'valid')
    data_format='channels_last',  # 데이터 형식 (기본값: 'channels_last')
    dilation_rate=1,  # 팽창 비율 (기본값: 1)
    activation=None,  # 활성화 함수 (기본값: None)
    use_bias=True,  # 바이어스 사용 여부 (기본값: True)
    kernel_initializer='glorot_uniform',  # 커널 초기화 방법 (기본값: 'glorot_uniform')
    bias_initializer='zeros',  # 바이어스 초기화 방법 (기본값: 'zeros')
    kernel_regularizer=None,  # 커널 정규화 방법 (기본값: None)
    bias_regularizer=None,  # 바이어스 정규화 방법 (기본값: None)
    activity_regularizer=None,  # 출력 정규화 방법 (기본값: None)
    kernel_constraint=None,  # 커널 제약 조건 (기본값: None)
    bias_constraint=None  # 바이어스 제약 조건 (기본값: None)
)
설명: 1차원 합성곱 층으로, 시퀀스 데이터의 특징을 추출하는 데 사용됩니다.

10. tf.keras.layers.GlobalAveragePooling1D
tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last'  # 데이터 형식 (기본값: 'channels_last')
)
설명: 시퀀스의 각 특징에 대해 평균을 계산하여 시퀀스의 길이를 줄이는 층입니다.

11. tf.keras.layers.GlobalMaxPooling1D
tf.keras.layers.GlobalMaxPooling1D(
    data_format='channels_last'  # 데이터 형식 (기본값: 'channels_last')
)
설명: 시퀀스의 각 특징에 대해 최대값을 계산하여 시퀀스의 길이를 줄이는 층입니다.












데이터 정규화는 데이터를 스케일링하거나 변환하여 딥러닝 모델이 더 잘 학습하도록 돕는 중요한 과정입니다. 데이터의 특성과 분포에 따라 사용할 수 있는 다양한 정규화 방법이 있습니다. 아래는 데이터의 특성에 따라 적합한 정규화 방법들을 정리한 내용입니다.

1. Min-Max 정규화 (Normalization)
방법: 데이터를 
0
,
1
0,1 (혹은 다른 사용자 지정 범위)로 스케일링.
 
특징:
값의 범위가 고정되어 상대적 크기를 학습하기 유리.
데이터가 일정한 최대/최소 범위를 가진 경우 적합.
적용 대상:
가격 데이터 (open, high, low, close)
확실한 최대/최소 범위가 있는 데이터 (볼린저 밴드, 이동 평균 등)
주의: 이상치(outlier)에 민감함.
2. Z-Score 표준화 (Standardization)
방법: 데이터를 평균 0, 표준편차 1로 변환.
 
(여기서 
𝜇
μ는 평균, 
𝜎
σ는 표준편차)
특징:
평균 중심의 데이터를 학습에 유리하게 만듦.
값의 분포가 넓거나 이상치가 있는 경우 적합.
적용 대상:
모멘텀 지표, MACD, CCI, ATR 등 분포가 넓은 데이터
주의: 데이터가 정규분포를 따를 때 더 효과적임.
3. 로그 변환 (Log Transformation)
방법: 데이터를 로그 함수로 변환.
특징:
값이 비대칭적으로 분포(오른쪽으로 치우침)된 경우 적합.
큰 값의 차이를 줄여주는 효과가 있음.
적용 대상:
거래량(volume), 소득, 클릭 수 등 양의 값이 큰 경우
주의: 음수 데이터에 바로 적용 불가.
4. MaxAbs 정규화
방법: 데이터를 
특징:
음수와 양수가 섞여 있는 데이터에 적합.
이상치에 민감하지 않음.
적용 대상:
양수와 음수가 공존하는 지표 (MACD, Williams %R 등)
5. Robust 정규화
방법: 중앙값(median)과 IQR(사분위 범위)을 사용하여 이상치의 영향을 줄임.
특징:
이상치(outlier)에 강건함.
값이 넓은 범위를 가지는 데이터에 적합.
적용 대상:
이상치가 많은 데이터 (거래량, 모멘텀 등)
6. 분위수 정규화 (Quantile Transformation)
방법: 데이터를 균일한 분포로 변환.
예를 들어, 데이터를 정규분포에 맞게 매핑하거나 균등 분포로 변환.
특징:
비대칭 분포를 균일하게 만듦.
적용 대상:
비정규 분포의 데이터
7. L2 정규화 (Vector Normalization)
방법: 벡터의 유클리드 길이를 1로 만듦.
특징:
벡터 크기를 균일하게 맞추는 데 유용.
적용 대상:
텍스트 데이터의 임베딩, 유사도 비교를 위한 데이터
8. 비닝 (Binning)
방법: 데이터를 구간(bin)으로 나누어 범주형 데이터처럼 다룸.
특징:
연속형 데이터를 범주형으로 변환하여 처리.
적용 대상:
값이 특정 구간에 집중되는 데이터
9. 단위 벡터 변환 (Unit Vector Scaling)
방법: 데이터를 단위 벡터의 형태로 변환.
특징:
특정 벡터 공간 내에서 상대적 크기를 유지.
적용 대상:
데이터의 방향 정보가 중요할 때
10. 기타 특수 변환
Box-Cox 변환:
데이터의 분포를 정규분포에 가깝게 변환.
적용 대상: 양수 데이터.
Yeo-Johnson 변환:
Box-Cox의 확장으로 음수 데이터에도 사용 가능.
클리핑(Clipping):
값의 범위를 제한 (이상치 제거).
적용 대상: 이상치가 극단적으로 많은 경우.
정규화 방법 선택 팁
데이터의 분포 확인: 데이터의 히스토그램, 박스 플롯 등을 사용해 분포를 분석.
이상치 여부 판단: 이상치가 많으면 Robust 정규화나 로그 변환.
분포의 대칭성: 비대칭적이면 로그 변환, Box-Cox, Yeo-Johnson.
값의 크기: 크기가 큰 경우 Min-Max, Z-Score, 로그 변환.
모델 특성:
딥러닝: Min-Max, Z-Score 권장.
트리 기반 모델: 정규화가 필요하지 않을 수도 있음.
궁금한 부분이 있으면 더 구체적으로 설명할게! 😊




1. 양수 데이터에서도 표준화를 사용하는 이유
딥러닝 모델이 원하는 것은 상대적인 스케일링이지, 값의 부호가 아님
표준화는 데이터를 평균 0, 표준편차 1로 변환해 각 피처의 값 분포를 조정하는 과정이야.
딥러닝 모델은 데이터 값의 절대적인 크기보다 상대적인 분포와 패턴에 민감하게 반응하기 때문에, 값이 음수가 되어도 학습에 전혀 문제가 없어.




1. 전체 raw_data를 정규화한 후 시퀀스 샘플을 자르는 방법
과정:
전체 데이터를 기준으로 정규화를 수행합니다.
이후, 정규화된 데이터를 시퀀스 샘플로 나눕니다.
장점:
데이터 간 일관성 유지:
모든 샘플이 동일한 정규화 기준(전체 데이터의 평균, 분산, 최소값, 최대값 등)을 공유하므로, 서로 다른 샘플 간 비교가 가능해집니다.
시계열 분석에 적합:
시퀀스 데이터가 시간에 따른 트렌드와 변화를 포함한다면, 전체 데이터의 범위를 고려하여 정규화하는 것이 더 자연스럽습니다.
안정적인 학습:
LSTM이나 Transformer 같은 모델은 데이터를 처리할 때 전체 데이터의 분포를 반영한 입력 값을 선호하므로, 학습 안정성이 높아질 수 있습니다.
단점:
새로운 데이터 적용 시 제한:
학습 중 정규화를 수행한 기준(예: 평균, 표준편차)을 저장해야 하며, 새로운 데이터가 들어왔을 때 동일한 기준으로 변환해야 합니다.
국소적 정보 손실 가능성:
시퀀스 샘플이 매우 작거나 특정 구간에 국한된 경우, 전체 데이터 기준으로 정규화하면 해당 구간의 특성이 희석될 수 있습니다.
2. 시퀀스 샘플로 나눈 후 각각 정규화하는 방법
과정:
전체 데이터를 시퀀스 샘플로 나눕니다.
각 샘플에 대해 독립적으로 정규화를 수행합니다.
장점:
로컬 특성 강조:
각 시퀀스 샘플의 범위와 분포를 기준으로 정규화하므로, 샘플의 로컬 특성이 학습에 더 잘 반영될 수 있습니다.
다양한 데이터 패턴에 적합:
시퀀스 데이터의 분포가 샘플마다 크게 다르다면, 개별 정규화가 모델 학습에 더 유리할 수 있습니다.
단점:
샘플 간 비교 어려움:
서로 다른 샘플이 다른 정규화 기준을 갖기 때문에, 모델이 시퀀스 간 관계를 학습하기 어려울 수 있습니다.
예를 들어, 하나의 샘플에서 값이 
0.9
0.9라면 이는 해당 샘플 내에서 높은 값이지만, 다른 샘플에서는 
0.1
0.1일 수도 있습니다.
글로벌 정보 손실:
전체 데이터의 분포를 학습하는 데 필요한 정보를 잃을 가능성이 있습니다.
추론 시 복잡성 증가:
새로운 데이터를 처리할 때, 각 시퀀스마다 정규화를 개별적으로 수행해야 하므로 계산 비용이 증가합니다.