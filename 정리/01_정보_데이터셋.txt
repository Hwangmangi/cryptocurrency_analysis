●0. tf.data
●1. tf.data.Dataset 사용
●2. TFRecord

====================================================================================================
●0. tf.data

TensorFlow에서 데이터셋을 다루는 것은 효율적이고 빠르게 모델을 훈련하기 위해 매우 중요합니다. 
TensorFlow 2.x에서는 특히 tf.data API를 사용하여 데이터를 처리하는 방법이 많이 사용됩니다. 
이는 대규모 데이터를 메모리 효율적으로 로드하고, 전처리하고, 모델에 공급하는데 최적화되어 있습니다.

(1)tf.data를 사용하는 이유
    -대규모 데이터 처리: 메모리에 전부 적재할 수 없는 데이터도 효율적으로 로드 가능.
    -데이터 증강 및 전처리: 데이터 로드 시 동적으로 변형 가능.
    -병렬 처리: 데이터 로드 및 전처리를 병렬로 수행하여 훈련 시간을 단축.
    -Batching & Prefetching: 미리 데이터를 로드하여 훈련이 중단 없이 이루어지도록 보장.

(2)tf.data.Dataset의 병렬 처리 및 비동기적 데이터 공급 방식
    TensorFlow의 tf.data API는 병렬 처리와 비동기 데이터 로드를 통해 모델 훈련을 가속화합니다. 
    이를 위해 내부적으로 **스레드와 큐(queue)**를 사용하여 데이터를 동적으로 생성하고, fit() 함수에서 필요할 때마다 데이터를 가져와 사용할 수 있도록 설계되었습니다.

    1. Prefetch를 통한 비동기 처리
        dataset.prefetch(buffer_size=tf.data.AUTOTUNE)는 fit() 함수가 훈련을 진행하는 동안 다음 배치 데이터를 미리 가져오는 비동기 작업을 수행합니다.
        이 작업은 별도의 스레드에서 이루어지기 때문에, 모델이 훈련 중일 때도 데이터 로드가 동시에 진행됩니다.
        AUTOTUNE 옵션을 사용하면 TensorFlow가 자동으로 최적의 buffer_size를 결정하여 성능을 최적화합니다.
    
    2. 병렬 데이터 로드 및 변환
        dataset.map()에서 num_parallel_calls=tf.data.AUTOTUNE 옵션을 사용하면, 데이터를 여러 스레드에서 병렬로 전처리할 수 있습니다.
        예를 들어, 이미지 데이터에 대한 증강이나 정규화 작업이 여러 스레드에서 동시에 처리되기 때문에 데이터 로드와 변환 속도가 크게 향상됩니다.

(3) tf.data.Dataset의 기본 형태 및 map함수 적용형태
    1.단일 텐서
        데이터셋의 각 요소가 단일 텐서로 구성됩니다.
        ┌─────────────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])
        # 출력: 1, 2, 3, 4, 5

        def square(x):
            return x ** 2
        # map 적용
        mapped_dataset = dataset.map(square)
        for element in mapped_dataset:
            print(element.numpy())  # 출력: 1, 4, 9, 16, 25
        └─────────────────────────────────────────────────────────────┘

    2. 텐서의 튜플
        데이터셋의 각 요소가 텐서의 튜플로 구성됩니다.
        예: (입력 데이터, 타겟 값) 또는 (이미지, 레이블)
        ┌─────────────────────────────────────────────────────────────┐
        inputs = [1, 2, 3, 4, 5]
        labels = [0, 1, 0, 1, 0]
        dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
        # 출력: (1, 0), (2, 1), (3, 0), ...

        def add_feature_label(feature, label):
            return feature + 1, label * 2
        # map 적용
        mapped_dataset = dataset.map(add_feature_label)
        for feature, label in mapped_dataset:
            print(f'Feature: {feature.numpy()}, Label: {label.numpy()}')
        # 출력:
        # Feature: 2, Label: 0
        # Feature: 3, Label: 2
        # Feature: 4, Label: 0
        # Feature: 5, Label: 2
        # Feature: 6, Label: 0
        └─────────────────────────────────────────────────────────────┘

    3.텐서의 딕셔너리
        데이터셋의 각 요소가 딕셔너리 형태의 텐서로 구성됩니다.
        예: {'feature1': ..., 'feature2': ...}
        ┌────────────────────────────────────────────────────┐
        features = {
            'feature1': [1, 2, 3],
            'feature2': [4, 5, 6],
        }
        dataset = tf.data.Dataset.from_tensor_slices(features)
        # 출력: {'feature1': 1, 'feature2': 0.1}, ...

        def modify_features(sample):
            return {
                "feature1": sample["feature1"] * 2,
                "feature2": sample["feature2"] + 3
            }
    
        # map 적용
        mapped_dataset = dataset.map(modify_features)
        for element in mapped_dataset:
            print(element)
        # 출력:
        # {'feature1': 2, 'feature2': 7}
        # {'feature1': 4, 'feature2': 8}
        # {'feature1': 6, 'feature2': 9}
        └────────────────────────────────────────────────────┘
====================================================================================================
●1. tf.data.Dataset 사용

1. Dataset 생성 함수
    1.1 from_tensor_slices
        역할: Python 리스트, 넘파이 배열, 텐서 등의 데이터로부터 Dataset을 생성합니다.
        ┌────────────────────────────────────────────────────┐
        import tensorflow as tf
        data = [1, 2, 3, 4, 5]
        dataset = tf.data.Dataset.from_tensor_slices(data)
        for item in dataset:
            print(item.numpy())
        └────────────────────────────────────────────────────┘


    1.2 from_generator
        역할: Python generator로부터 Dataset을 생성합니다.
        ┌──────────────────────────────────────────────────────────────────────────────┐
        def generator():
           for i in range(5):
               yield i * i

        dataset = tf.data.Dataset.from_generator(generator, output_types=tf.int32)
        for item in dataset:
         print(item.numpy())
        └──────────────────────────────────────────────────────────────────────────────┘


    1.3 range
        역할: 주어진 범위의 정수를 포함하는 Dataset을 생성합니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(5)
        for item in dataset:
            print(item.numpy())
        └────────────────────────────────────────────────────┘


    1.4 TFRecordDataset
        역할: TFRecord 파일로부터 데이터를 읽어 Dataset을 생성합니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.TFRecordDataset(["data.tfrecord"])
        └────────────────────────────────────────────────────┘


    1.5 TextLineDataset
        역할: 텍스트 파일에서 각 줄을 읽어 Dataset을 생성합니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.TextLineDataset("file.txt")
        └────────────────────────────────────────────────────┘

2. Dataset 변환 함수
    ★2.1 map
        역할: 데이터셋의 각 요소에 대해 주어진 함수를 적용합니다.

        사용법 : mapped_dataset = dataset.map(map_func, num_parallel_calls=tf.data.AUTOTUNE)
        -dataset: 변환을 적용할 TensorFlow 데이터셋 객체 (tf.data.Dataset).
        -map_func: 데이터셋의 각 요소에 적용할 함수.
        -num_parallel_calls: 병렬 처리를 위한 워커 수 (기본값은 None).
            tf.data.AUTOTUNE을 사용하면 TensorFlow가 적절한 병렬 작업 개수를 자동으로 결정.

        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(5)
        dataset = dataset.map(lambda x: x * x)
        for item in dataset:
            print(item.numpy())
        └────────────────────────────────────────────────────┘
    
    2.2 filter
        역할: 조건을 만족하는 데이터만 남깁니다.

        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(10)
        dataset = dataset.filter(lambda x: x % 2 == 0)
        for item in dataset:
            print(item.numpy())
        └────────────────────────────────────────────────────┘
    2.3 batch
        역할: 데이터셋을 지정된 크기로 묶습니다.

        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(10)
        dataset = dataset.batch(3)
        for batch in dataset:
            print(batch.numpy())
        └────────────────────────────────────────────────────┘
    2.4 shuffle
        역할: 데이터를 무작위로 섞습니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(10)
        dataset = dataset.shuffle(buffer_size=5)
        for item in dataset:
            print(item.numpy())
        └────────────────────────────────────────────────────┘

    2.5 repeat
        역할: 데이터셋을 지정된 횟수만큼 반복합니다. 인수를 생략하면 무한 반복합니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(5).repeat(2)
        for item in dataset:
            print(item.numpy())
        └────────────────────────────────────────────────────┘

    2.6 prefetch
        역할: 데이터 로딩과 처리 병렬화를 위해 다음 데이터를 미리 로드합니다.
        ┌────────────────────────────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(10).prefetch(buffer_size=tf.data.AUTOTUNE)
        └────────────────────────────────────────────────────────────────────────────┘


3. Dataset 소비 함수
    3.1 take
        역할: 데이터셋의 첫 n개의 요소를 가져옵니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(10)
        for item in dataset.take(3):
            print(item.numpy())
        └────────────────────────────────────────────────────┘
    
    3.2 enumerate
        역할: 데이터셋의 각 요소에 인덱스를 추가합니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(3)
        dataset = dataset.enumerate(start=100)
        for index, value in dataset:
            print(f'Index: {index.numpy()}, Value: {value.numpy()}')
        └────────────────────────────────────────────────────┘
    

    3.3 as_numpy_iterator
        역할: Dataset을 numpy 형식으로 순회할 수 있는 iterator로 변환합니다.
        ┌────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(5)
        for item in dataset.as_numpy_iterator():
            print(item)
        └────────────────────────────────────────────────────┘
    

4. 고급 함수
    ★4.1 window
        역할: 데이터셋을 고정 크기의 윈도우로 나눕니다.
        ┌─────────────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(10)
        dataset = dataset.window(size=3, shift=1, drop_remainder=True)
        for window in dataset:
            print(list(window.as_numpy_iterator()))
        └─────────────────────────────────────────────────────────────┘
    
    4.2 flat_map
        역할: 데이터셋의 각 요소에 대해 새로운 데이터셋을 생성하고 이를 하나의 데이터셋으로 병합합니다.
        ┌────────────────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(3)
        dataset = dataset.flat_map(lambda x: tf.data.Dataset.range(x))
        for item in dataset:
            print(item.numpy())
        └────────────────────────────────────────────────────────────────┘
    
    4.3 interleave
        역할: 여러 데이터셋을 병렬로 혼합합니다.
        ┌───────────────────────────────────────────────────────────────────────────────┐
        dataset = tf.data.Dataset.range(3)
        dataset = dataset.interleave(lambda x: tf.data.Dataset.range(x), cycle_length=2)
        for item in dataset:
            print(item.numpy())
        └───────────────────────────────────────────────────────────────────────────────┘

    4.4 concatenate
        TensorFlow의 tf.data.Dataset을 사용할 때, 여러 데이터셋을 합칠 때는 concatenate() 함수를 사용할 수 있습니다.
        combined_dataset = dataset1.concatenate(dataset2)의 형태
        ┌──────────────────────────────────────────────────────────────────────┐
        import tensorflow as tf
        # 예시 데이터셋 1
        dataset1 = tf.data.Dataset.from_tensor_slices([1, 2, 3])
        # 예시 데이터셋 2
        dataset2 = tf.data.Dataset.from_tensor_slices([4, 5, 6])
        # 데이터셋 합치기
        combined_dataset = dataset1.concatenate(dataset2)
        # 출력
        for item in combined_dataset:
            print(item.numpy())
        └──────────────────────────────────────────────────────────────────────┘

# 5. 타겟값과 함께 데이터셋을 만들기
#     ┌───────────────────────────────────────────────────────────────────────────────┐
#     import tensorflow as tf

#     # 예시 데이터
#     data = [1, 2, 3, 4, 5]  # 특징값
#     targets = [0, 1, 0, 1, 0]  # 타겟값

#     # 데이터셋 생성
#     dataset = tf.data.Dataset.from_tensor_slices((data, targets))

#     # 데이터셋 사용
#     for x, y in dataset:
#         print(f'Feature: {x.numpy()}, Target: {y.numpy()}')
#     └───────────────────────────────────────────────────────────────────────────────┘

★6. 데이터셋 저장하고 불러오기
    (1) TFRecord로 데이터셋 저장하기
        ┌───────────────────────────────────────────────────────────────────────────────┐
        import tensorflow as tf
        # 예시 데이터셋
        data = [1, 2, 3, 4, 5]
        labels = [0, 1, 0, 1, 0]
        # 데이터셋 생성
        dataset = tf.data.Dataset.from_tensor_slices((data, labels))
        # TFRecord 저장을 위한 직렬화 함수
        def serialize_example(feature0, feature1):
            feature = {
                'feature0': tf.train.Feature(int64_list=tf.train.Int64List(value=[feature0])),
                'feature1': tf.train.Feature(int64_list=tf.train.Int64List(value=[feature1])),
            }
            example = tf.train.Example(features=tf.train.Features(feature=feature))
            return example.SerializeToString()

        # TFRecord 파일로 저장
        with tf.io.TFRecordWriter('dataset.tfrecord') as writer:
            for feature0, feature1 in dataset: 
                example = serialize_example(feature0.numpy(), feature1.numpy())
                writer.write(example)
        └───────────────────────────────────────────────────────────────────────────────┘
    
    (2) TFRecord 파일에서 데이터셋 불러오기(실제 학습에 사용할때는 뭔가 더 수정을 해야한다)
        ┌───────────────────────────────────────────────────────────────────────────────┐
        import tensorflow as tf
        # TFRecord 파일에서 데이터셋 불러오기
        raw_dataset = tf.data.TFRecordDataset('dataset.tfrecord')
        # 파싱 함수 정의
        def _parse_function(proto):
            # 데이터셋에서 사용된 특성 정의
            keys_to_features = {
                'feature0': tf.io.FixedLenFeature([], tf.int64),
                'feature1': tf.io.FixedLenFeature([], tf.int64)
            }
            parsed_features = tf.io.parse_single_example(proto, keys_to_features)
            return parsed_features['feature0'], parsed_features['feature1']
    
        # 데이터셋 파싱
        dataset = raw_dataset.map(_parse_function)
        # 데이터셋 출력
        for item in dataset:
            print(item)
        └───────────────────────────────────────────────────────────────────────────────┘

    (3) 관련 함수 정리
        1). tf.train.Feature
            tf.train.Feature는 데이터를 TFRecord 형식으로 저장하기 위해 사용되는 기본 데이터 구조입니다.
            데이터를 직렬화(serialize)하거나 복원(deserialize)하는 데 사용됩니다.
            데이터는 3가지 타입만 지원합니다:
            -int64_list: 정수(int64) 데이터를 표현.
            -float_list: 부동소수(float32/float64) 데이터를 표현.
            -bytes_list: 문자열 또는 바이트 데이터를 표현.
            ┌─────────────────────────┐
            tf.train.Feature(
            bytes_list=None,
                float_list=None,
                int64_list=None
            )
            └────────────────────────┘
            -bytes_list: tf.train.BytesList 객체, 문자열/바이트 데이터를 담습니다.
            -float_list: tf.train.FloatList 객체, 부동소수 데이터를 담습니다.
            -int64_list: tf.train.Int64List 객체, 정수 데이터를 담습니다.
            ┌───────────────────────────────────────────────────────────┐
            feature = tf.train.Feature(
            int64_list=tf.train.Int64List(value=[10])  # 정수값 10 저장
            )
            └───────────────────────────────────────────────────────────┘

        2). tf.train.Int64List, tf.train.FloatList, tf.train.BytesList
            각각 tf.train.Feature의 데이터 타입을 정의하는 클래스입니다.
            데이터를 TFRecord 파일에 저장하기 위해 각 타입으로 변환합니다.
            ┌───────────────────────────────────────────────────────────┐
            # 정수 리스트
            tf.train.Int64List(value=[1, 2, 3])
            # 부동소수점 리스트
            tf.train.FloatList(value=[1.1, 2.2, 3.3])
            # 바이트/문자열 리스트
            tf.train.BytesList(value=["hello".encode(), "world".encode()])
            └───────────────────────────────────────────────────────────┘
            -value: 저장할 값의 리스트입니다. 리스트 형식이어야 합니다.
            -정수: tf.train.Int64List
            -부동소수점: tf.train.FloatList
            -문자열/바이트: tf.train.BytesList

            ┌───────────────────────────────────────────────────────────┐
            int_list = tf.train.Int64List(value=[10, 20, 30])
            float_list = tf.train.FloatList(value=[1.5, 2.5])
            bytes_list = tf.train.BytesList(value=["example".encode()])
            └───────────────────────────────────────────────────────────┘

        3). tf.train.Features
            여러 개의 tf.train.Feature를 묶어서 하나의 객체로 만듭니다.
            딕셔너리 형태로 Feature를 관리합니다.
            ┌─────────────────────┐
            tf.train.Features(
            feature=None
            )
            └─────────────────────┘
            -feature: 딕셔너리 형태로 key-value 쌍을 가집니다.
            -key: 문자열 이름 (데이터의 이름을 나타냄).
            -value: tf.train.Feature 객체.

            ┌──────────────────────────────────────────────────────────────────────────────────────────┐
            features = tf.train.Features(
                feature={
                    'age': tf.train.Feature(int64_list=tf.train.Int64List(value=[25])),
                    'height': tf.train.Feature(float_list=tf.train.FloatList(value=[5.8])),
                    'name': tf.train.Feature(bytes_list=tf.train.BytesList(value=["Alice".encode()]))
                }
            )
            └──────────────────────────────────────────────────────────────────────────────────────────┘


        4). tf.train.Example
            tf.train.Features를 포함하는 최상위 데이터 구조로, 데이터를 TFRecord 형식으로 직렬화하기 위해 사용됩니다.
            Example은 TensorFlow에서 직렬화된 데이터를 저장하고 불러오는 데 필요한 구조입니다.
            ┌──────────────────────┐
            tf.train.Example(
                features=None
            )
            └─────────────────────┘
            -features: tf.train.Features 객체를 받습니다.

            ┌───────────────────────────────────────────────────────────────────────────────────────────┐
            example = tf.train.Example(
                features=tf.train.Features(
                    feature={
                        'age': tf.train.Feature(int64_list=tf.train.Int64List(value=[25])),
                        'height': tf.train.Feature(float_list=tf.train.FloatList(value=[5.8])),
                        'name': tf.train.Feature(bytes_list=tf.train.BytesList(value=["Alice".encode()]))
                    }
                )
            )
            └───────────────────────────────────────────────────────────────────────────────────────────┘

        5). tf.io.TFRecordWriter
            데이터를 TFRecord 파일에 저장하는 데 사용되는 클래스입니다.
            직렬화된 데이터(SerializeToString으로 변환된 데이터)를 받아 파일로 저장합니다.
            ┌─────────────────────────┐
            tf.io.TFRecordWriter(
                path
            )
            └─────────────────────────┘
            -path: 데이터를 저장할 TFRecord 파일의 경로입니다.

            ┌───────────────────────────────────────────────────────────┐
            with tf.io.TFRecordWriter('example.tfrecord') as writer:
                serialized_example = example.SerializeToString()
                writer.write(serialized_example)
            └───────────────────────────────────────────────────────────┘

        6). tf.io.FixedLenFeature
            TFRecord 파일에서 데이터를 읽을 때, 데이터의 고정 길이와 타입을 정의합니다.
            데이터 파싱 시, TensorFlow에 데이터 구조를 알려주는 역할을 합니다.
            ┌──────────────────────────┐
            tf.io.FixedLenFeature(
                shape,
                dtype,
                default_value=None
            )
            └──────────────────────────┘
            -shape: 데이터의 고정된 크기(형태). 빈 리스트([])는 스칼라 값을 의미합니다.
            -dtype: 데이터 타입 (tf.float32, tf.int64, tf.string).
            -default_value (선택): 값이 없을 경우 기본값을 설정합니다.

            ┌───────────────────────────────────────────────────────┐
            keys_to_features = {
                'age': tf.io.FixedLenFeature([], tf.int64),
                'height': tf.io.FixedLenFeature([], tf.float32),
                'name': tf.io.FixedLenFeature([], tf.string)
            }
            └───────────────────────────────────────────────────────┘

        7). tf.io.parse_single_example
            TFRecord 파일에 저장된 직렬화 데이터를 **파싱(복원)**하는 함수입니다.
            데이터를 읽고, tf.io.FixedLenFeature로 정의된 구조에 따라 변환합니다.
            ┌─────────────────────────────┐
            tf.io.parse_single_example(
            serialized,
            features
            )
            └─────────────────────────────┘
            -serialized: 직렬화된 데이터 (TFRecord 파일에서 읽은 데이터).
            -features: 데이터를 복원할 구조를 정의한 딕셔너리 (tf.io.FixedLenFeature로 정의).
            -리턴값 : 파싱된 데이터가 딕셔너리로 반환됩니다.

            ┌───────────────────────────────────────────────────────────────────────────────┐
            serialized_example = example.SerializeToString()
            # 파싱 구조 정의
            keys_to_features = {
                'age': tf.io.FixedLenFeature([], tf.int64),
                'height': tf.io.FixedLenFeature([], tf.float32),
                'name': tf.io.FixedLenFeature([], tf.string)
            }   
            # 직렬화된 데이터를 파싱
            parsed_example = tf.io.parse_single_example(serialized_example, keys_to_features)
            print(parsed_example)
            └───────────────────────────────────────────────────────────────────────────────┘

7. 실제 학습시 예시 코드
    ┌───────────────────────────────────────────────────────────────────────────────┐
    import tensorflow as tf

    # TFRecord 파일 경로
    tfrecord_file = 'dataset.tfrecord'

    # 1. TFRecordDataset으로 데이터 로드
    raw_dataset = tf.data.TFRecordDataset(tfrecord_file)

    # 2. 데이터 파싱 함수 정의
    def parse_example(proto):
        # TFRecord 파일의 특성 정의
        feature_description = {
            'feature0': tf.io.FixedLenFeature([], tf.int64),
            'feature1': tf.io.FixedLenFeature([], tf.int64),
        }
        parsed_features = tf.io.parse_single_example(proto, feature_description)
        # 입력 (feature0)과 레이블 (feature1)을 반환
        return parsed_features['feature0'], parsed_features['feature1']

    # 3. 데이터셋 파싱
    parsed_dataset = raw_dataset.map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)

    # 4. 데이터 전처리
    def preprocess(feature, label):
        # 입력 데이터를 float으로 변환
        feature = tf.cast(feature, tf.float32)
        # 레이블은 이미 적합하므로 그대로 사용
        return feature, label

    processed_dataset = parsed_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)

    # 5. 데이터셋 셔플, 배치, prefetch
    final_dataset = (processed_dataset
                 .shuffle(buffer_size=1000)           # 셔플 버퍼 크기
                 .batch(32)                           # 배치 크기
                 .prefetch(buffer_size=tf.data.AUTOTUNE))  # Prefetch로 데이터 로딩 최적화

    # 6. 모델 정의
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(16, activation='relu', input_shape=(1,)),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    # 7. 모델 컴파일
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # 8. 모델 학습
    model.fit(final_dataset, epochs=5)
    └───────────────────────────────────────────────────────────────────────────────┘


====================================================================================================
 ●2. TFRecord
 TFRecord는 TensorFlow에서 데이터를 저장하고 처리하기 위해 설계된 바이너리 파일 형식입니다.
대규모 데이터를 효율적으로 저장하고 읽어오기 위해 사용되며, 텍스트나 CSV와 같은 파일 형식보다 읽기/쓰기 성능이 뛰어나고, 확장성이 좋습니다.


(1) TFRecord의 특징
    -이진 형식(Binary Format):
        사람이 읽을 수 있는 형식이 아니라, 데이터가 이진(binary) 형식으로 저장됩니다.
        이로 인해 데이터 크기가 작아지고 입출력 속도가 빨라집니다.

    -구조화된 데이터 저장:
        하나의 TFRecord 파일에 여러 개의 데이터(샘플)를 저장할 수 있습니다.
        예를 들어, 이미지 데이터와 레이블을 함께 저장하거나, 여러 속성을 포함하는 구조화 데이터를 저장할 수 있습니다.

    -데이터 효율성:
        CSV와 같은 텍스트 기반 파일은 용량이 크고 파싱 속도가 느린 반면, TFRecord는 데이터를 직렬화(serialize) 하여 저장하므로 더 적은 메모리와 디스크 공간을 사용합니다.

    -TensorFlow와의 높은 호환성:
        TensorFlow의 데이터 파이프라인(tf.data)과 통합되어 있어 대규모 데이터를 효율적으로 읽고 처리할 수 있습니다.
       

(2) TFRecord의 구조
    TFRecord 파일은 샘플 데이터를 순차적으로 저장합니다.
    각 데이터 샘플은 **Protocol Buffer(프로토콜 버퍼, Protobuf)**를 사용하여 직렬화됩니다.
        
    =주요 구성 요소
        1.tf.train.Feature:
            데이터를 저장하는 기본 단위입니다.
            데이터는 세 가지 형식 중 하나로 저장됩니다:
            -BytesList: 문자열 또는 바이트 데이터 (예: 이미지 파일, 텍스트).
            -FloatList: 부동소수점 데이터 (예: 실수형 데이터).
            -Int64List: 정수 데이터 (예: 카테고리, 레이블).
       
        2.tf.train.Features:
            여러 개의 tf.train.Feature를 묶어서 하나의 데이터 샘플을 만듭니다.

        3.tf.train.Example: 
            단일 데이터 샘플을 포함하는 구조체.
            tf.train.Features를 포함하여 데이터 샘플을 구성합니다.  

(3) Protocol Buffer(프로토콜 버퍼)란?
    **Protocol Buffer(ProtoBuf)**는 Google이 개발한 직렬화 및 역직렬화 라이브러리입니다.
    주로 데이터를 저장하거나 네트워크를 통해 데이터를 전송할 때 사용됩니다.
            
    1.핵심 개념
        -직렬화(Serialization):
            데이터를 저장하거나 전송하기 위해 바이너리 형식으로 변환하는 과정.
            예를 들어, 사람이 읽을 수 있는 JSON이나 XML 같은 데이터를 더 효율적인 바이너리 형식으로 변환.
            
        -역직렬화(Deserialization):
            바이너리 형식을 다시 사람이 이해할 수 있는 데이터로 복원하는 과정.
           
    2.Protocol Buffer의 특징
        -효율성:
            데이터 크기가 작고, 직렬화/역직렬화 속도가 빠릅니다.
            (JSON, XML보다 약 3~10배 더 빠르고 크기가 작음.)
            
        -다양한 언어 지원:
            Python, C++, Java 등 다양한 언어에서 사용할 수 있습니다.
           
        -구조화된 데이터 지원:
            정수, 문자열, 배열, 객체 등 다양한 데이터 형식을 지원합니다.
            데이터를 정의하는 .proto 파일을 사용하여 구조를 명확히 설계할 수 있음.

